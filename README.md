# ğŸ§  Research Agent CLI

A polished, interactive command-line research assistant that searches the web and delivers structured, source-backed answers â€” powered by **Kimi K2** (via Groq), **Tavily** web search, and **LangGraph**.

## What It Does

You ask a question in plain English. The agent autonomously decides whether it needs to search the web, executes one or more searches, reads the results, reasons about them, and returns a structured answer with:

- **TL;DR** â€” 2-bullet summary
- **Key Points** â€” 5 detailed bullets
- **Sources** â€” clickable URLs for every claim

All of this happens in real time with animated spinners and color-coded panels in your terminal.

## How It Works

### The ReAct Pattern

This agent uses the **ReAct** (Reason + Act) pattern, a well-known approach in AI agent design. Instead of answering in one shot, the LLM operates in a loop:

```
User Question
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  REASON              â”‚  â† LLM thinks about what it knows
â”‚  "I need current data"  â”‚     and what it still needs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”§ ACT                 â”‚  â† LLM calls a tool (web_search)
â”‚  web_search("query")    â”‚     with a self-chosen query
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“¥ OBSERVE             â”‚  â† LLM reads the search results
â”‚  [{title, url, snippet}]â”‚     and decides: enough info?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     Enough info? â”€â”€Noâ”€â”€â†’ Loop back to REASON
           â”‚
          Yes
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… ANSWER              â”‚  â† LLM synthesizes a final
â”‚  TL;DR + Key Points     â”‚     structured response
â”‚  + Sources              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This loop is managed by **LangGraph**, which orchestrates the state machine and handles message passing between the LLM and tools.

### Key Design Decisions

| Decision | Why |
|---|---|
| **Kimi K2 via Groq** | Kimi K2 is a strong reasoning model; Groq provides fast inference via their LPU hardware |
| **Tavily for search** | Purpose-built for AI agents â€” returns clean, structured results (not raw HTML) |
| **LangGraph ReAct** | Battle-tested agent loop with built-in state management and streaming |
| **Rich terminal UI** | Spinners, panels, and Markdown rendering make the experience feel professional |
| **Lazy agent init** | Agent is created on first use (`@lru_cache`), not at import time â€” avoids slow startup |
| **Trace logging** | Every run is saved to `outputs/` so you can audit what the agent did |

### Component Flow

```
main.py
  â””â†’ dotenv loads .env (API keys)
  â””â†’ cli/app.py
       â””â†’ Shows banner, starts REPL
       â””â†’ core/tracer.py
            â””â†’ agent.py (lazy init)
            â”‚    â””â†’ ChatGroq (Kimi K2)
            â”‚    â””â†’ tools/search.py (Tavily)
            â””â†’ Streams chunks from agent
            â””â†’ Displays rich panels
            â””â†’ Saves trace to outputs/
```

## Features

- ğŸ” **Web-grounded answers** â€” every factual claim is backed by a live search
- ğŸ§  **Autonomous reasoning** â€” the LLM decides when and what to search
- âœ¨ **Rich terminal UI** â€” spinners, coloured panels, Markdown-rendered answers
- ğŸ“ **Automatic trace logging** â€” every run is saved with timestamps
- ğŸ›¡ï¸ **Error resilience** â€” API failures return error JSON instead of crashing
- ğŸ§ª **4 E2E tests** â€” happy path, empty results, malformed data, API failure

## Quick Start

### 1. Install

```bash
git clone git@github.com:rezacr588/research-agent.git
cd research-agent
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure

Create a `.env` file in the project root (auto-loaded, no need to export):

```env
GROQ_API_KEY=your_groq_key_here
TAVILY_API_KEY=your_tavily_key_here
```

Get your keys:
- **Groq**: [console.groq.com](https://console.groq.com)
- **Tavily**: [tavily.com](https://tavily.com) (free tier available)

### 3. Run

```bash
python main.py
# or
python -m research_agent
```

### 4. Ask Questions

```
> What are the latest breakthroughs in quantum computing?
> Who won the Nobel Prize in Physics 2024?
> Compare React vs Vue for building web apps
```

| Command | Action |
|---|---|
| `quit` / `exit` / `q` | Exit |
| `clear` | Clear screen |
| `Ctrl+C` | Exit gracefully |

## Project Structure

```
research-agent/
â”œâ”€â”€ main.py                          # Entry point (loads .env)
â”œâ”€â”€ .env                             # API keys (git-ignored)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ research_agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py                  # python -m research_agent
â”‚   â”œâ”€â”€ agent.py                     # Lazy LLM factory + ReAct agent
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ search.py                # Tavily tool (cached client, error handling)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tracer.py                # Rich tracer (spinners, panels, file logging)
â”‚   â””â”€â”€ cli/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ app.py                   # Interactive REPL with rich prompt
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_agent.py                # 4 E2E tests (mocked APIs)
â””â”€â”€ outputs/                         # Auto-generated trace files
```

### Architecture Layers

| Layer | Responsibility | Key Design |
|---|---|---|
| **CLI** (`cli/`) | User interaction, env checks | `rich` panels, lazy imports |
| **Core** (`core/`) | Tracing, streaming, file logging | `PLAIN_OUTPUT` toggle for tests |
| **Agent** (`agent.py`) | LLM + ReAct loop config | `@lru_cache` factory avoids cold start |
| **Tools** (`tools/`) | External API calls | Cached client, defensive `.get()`, error JSON |

Dependencies flow inward: **CLI â†’ Core â†’ Agent â†’ Tools**

## Testing

```bash
python -m unittest tests.test_agent -v
```

Tests set `PLAIN_OUTPUT=1` to bypass rich formatting and mock the Tavily API â€” no network or keys needed.

| Test | Verifies |
|---|---|
| `test_agent_e2e_flow` | Full loop: question â†’ search â†’ structured answer |
| `test_empty_search_results` | Agent handles zero results without crashing |
| `test_malformed_search_results` | Missing fields don't cause `KeyError` |
| `test_search_api_failure` | API errors return error JSON, agent still responds |

## Environment Variables

| Variable | Required | Description |
|---|---|---|
| `GROQ_API_KEY` | âœ… | Groq API key for Kimi K2 inference |
| `TAVILY_API_KEY` | âœ… | Tavily API key for web search |
| `PLAIN_OUTPUT` | âŒ | Set to `1` for plain-text output (used in tests) |
